{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682dd481",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab25ac",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecdb00",
   "metadata": {},
   "source": [
    "- It is a supervised learning algorithm.\n",
    "- It is used for classification and regression tasks.\n",
    "- It works by recursively partitioning the dataset into subsets based on feature values.\n",
    "- It aims to produce the most homogeneous target groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fa73a",
   "metadata": {},
   "source": [
    "### Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a237e",
   "metadata": {},
   "source": [
    "#### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adafd5",
   "metadata": {},
   "source": [
    "- **Root Node** - Represents the entire dataset; the first split point.\n",
    "- **Decision Node** - Internal node where the dataset is split based on a condition.\n",
    "- **Leaf Node/Terminal Node** - Represents the output prediction.\n",
    "- **Branches** - Paths connecting nodes based on feature conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7a920",
   "metadata": {},
   "source": [
    "#### Features of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29ec71",
   "metadata": {},
   "source": [
    "- **Non-parametric** – Decision trees do not assume the data follows a specific probability distribution (like normal distribution).\n",
    "- **White-box model** – A decision tree’s logic is interpretable; you can trace exactly why a prediction was made.\n",
    "- Works with **numerical and categorical features**.\n",
    "- Handles **non-linear relationships** naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d9302",
   "metadata": {},
   "source": [
    "### Splitting Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966e7ee",
   "metadata": {},
   "source": [
    "For classification:\n",
    "- **Gini Impurity**  \n",
    "  - It is a measure of impurity or randomness used in decision tree algorithms.\n",
    "  - The Gini impurity for a node *t* is given by:  \n",
    "\n",
    "    $$\n",
    "    Gini(t) = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "    $$\n",
    "\n",
    "    Where:\n",
    "    - $t$ = The node (or split) being evaluated  \n",
    "    - $C$ = Total number of classes  \n",
    "    - $p_i$ = Proportion of class $i$ in node $t$\n",
    "  - *Interpretation*:\n",
    "    - Closer to 0: Node is pure --> good for decision-making.\n",
    "    - Closer to max value (0.5 for binary, 0.67 for 3 classes, etc): Node is highly mixed --> bad for classification.\n",
    "\n",
    "- **Entropy (Information Gain)**\n",
    "  - Entropy measures the impurity or disorder in a dataset.\n",
    "  \n",
    "  - A perfectly pure node (all samples belong to one class) has an entropy of 0, while a highly mixed node has higher entropy.\n",
    "  - Information Gain quantifies the reduction in entropy achieved after splitting the data based on a particular feature.\n",
    "  - In decision trees, the split with the highest Information Gain is chosen to create the most homogeneous child nodes.\n",
    "\n",
    "    $$\n",
    "    Entropy(t) = - \\sum_{i=1}^{C} p_i \\log_{2} p_i\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    InformationGain = Entropy_{parent} - \\sum_{k} \\frac{N_k}{N} \\, Entropy_{child_k}\n",
    "    $$\n",
    "\n",
    "    Where:  \n",
    "    - $Entropy_{parent}$ = Entropy before the split  \n",
    "    - $Entropy_{child_k}$ = Entropy of child node $k$  \n",
    "    - $N_k$ = Number of samples in child node $k$  \n",
    "    - $N$ = Total number of samples in the parent node\n",
    "\n",
    "For **Regression**:\n",
    "\n",
    "- **Variance Reduction**  \n",
    "  - Measures how much variance decreases after a split.\n",
    "\n",
    "- **Mean Squared Error (MSE)**  \n",
    "  - MSE measures the average of the squared differences between actual and predicted values.\n",
    "    $$\n",
    "    MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^2\n",
    "    $$\n",
    "    Where:  \n",
    "    - $n$ = Number of observations  \n",
    "    - $y_i$ = Actual value  \n",
    "    - $\\hat{y}$ = Predicted value  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16355c",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606b546",
   "metadata": {},
   "source": [
    "1. Start with the entire dataset.\n",
    "2. For each feature:\n",
    "    - Evaluate the split criterion (Gini, Entropy, MSE).\n",
    "3. Select the best split that maximizes impurity reduction.\n",
    "4. Partition data into subsets.\n",
    "5. Repeat recursively until:\n",
    "    - Pure nodes (all same class)\n",
    "    - Max depth reached\n",
    "    - Min samples per leaf reached\n",
    "    - No further improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee9af0",
   "metadata": {},
   "source": [
    "### Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7625a44",
   "metadata": {},
   "source": [
    "**Goal**: At each split, choose the feature $X_j$ and threshold $s$ that minimizes node impurity.\n",
    "$$\n",
    "\\underset{j,\\,s}{\\arg\\min} \n",
    "\\left[ \n",
    "\\frac{N_{\\text{left}}}{N} \\, I(\\text{left}) + \n",
    "\\frac{N_{\\text{right}}}{N} \\, I(\\text{right}) \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $I(.)$ - impurity function\n",
    "- $N$ - number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc25598",
   "metadata": {},
   "source": [
    "### Hyperparameters (Tuning & Controlling Overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae88c82",
   "metadata": {},
   "source": [
    "- **max_depth** – Limit tree depth.\n",
    "- **min_samples_split** – Minimum samples to split an internal node.\n",
    "- **min_samples_leaf** – Minimum samples at a leaf node.\n",
    "- **max_features** – Number of features to consider at each split.\n",
    "- **criterion** – `'gini'`, `'entropy'` for classification; `'mse'`, `'mae'` for regression.\n",
    "- **max_leaf_nodes** – Limit number of leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235dd8ea",
   "metadata": {},
   "source": [
    "### Handling Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01270135",
   "metadata": {},
   "source": [
    "- **Pre-Pruning** (set constraints before training: `max_depth`, `min_samples_leaf`, etc.)\n",
    "- **Post-Pruning** (train fully, then prune branches with minimal contribution — *cost-complexity pruning*).\n",
    "- Use **cross-validation** to find optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c13086",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffacef1",
   "metadata": {},
   "source": [
    "- Simple to interpret & visualize.\n",
    "- Works without feature scaling.\n",
    "- Can handle mixed feature types.\n",
    "- Captures non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188a569",
   "metadata": {},
   "source": [
    "### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65944f14",
   "metadata": {},
   "source": [
    "- High variance (overfits if uncontrolled).\n",
    "- Unstable with small changes in data.\n",
    "- Biased towards features with many categories.\n",
    "- Poor at extrapolation in regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8dddf8",
   "metadata": {},
   "source": [
    "### Advanced Decision Tree Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35cb3d",
   "metadata": {},
   "source": [
    "- **CART (Classification and Regression Trees)** – Most common implementation (binary splits).\n",
    "- **ID3 / C4.5 / C5.0** – Older algorithms using information gain.\n",
    "- **CHAID** – Uses chi-square tests for splitting.\n",
    "- **Oblique Decision Trees** – Splits on linear combinations of features.\n",
    "- **Probabilistic Trees** – Predict probabilities instead of hard labels.\n",
    "- **Gradient Boosted Trees & Random Forests** – Ensemble improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17275c1d",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc48ebf",
   "metadata": {},
   "source": [
    "For classification:\n",
    "- Accuracy, Precision, Recall, F1-score, AUROC.\n",
    "\n",
    "For regression:\n",
    "- RMSE, MAE, $R^2$\n",
    "\n",
    "**Validation Techniques**:\n",
    "- **k-Fold Cross-Validation**\n",
    "- **Stratified Sampling** (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d06a7",
   "metadata": {},
   "source": [
    "### Interpretability Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda59b86",
   "metadata": {},
   "source": [
    "- **Tree Visualization** (`sklearn.tree.plot_tree`, Graphviz)\n",
    "- **Feature Importance** (`.feature_importances_`)\n",
    "- **Decision Paths** (`decision_path()` in scikit-learn)\n",
    "- **SHAP Values** – For feature contribution explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6d4db",
   "metadata": {},
   "source": [
    "### Practical Implementation (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac837980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(clf, feature_names=feature_names, class_names=class_labels, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab8d9a",
   "metadata": {},
   "source": [
    "### Deployment Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfe212",
   "metadata": {},
   "source": [
    "- Store tree model as `.pkl` for reuse.\n",
    "- Beware of data drift — retrain periodically.\n",
    "- For interpretability in production, keep visualization updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d13a1",
   "metadata": {},
   "source": [
    "### Decision Tree in the Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c20ce",
   "metadata": {},
   "source": [
    "1. Data Preprocessing\n",
    "    - Handle missing values\n",
    "    - Encode categorical features\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Model Selection\n",
    "4. Hyperparameter Optimization (GridSearchCV / RandomizedSearchCV / Bayesian Optimization)\n",
    "5. Model Evaluation\n",
    "6. Interpretability Reporting\n",
    "7. Deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
